{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7256669,"sourceType":"datasetVersion","datasetId":4205123},{"sourceId":7268101,"sourceType":"datasetVersion","datasetId":4213084},{"sourceId":7504735,"sourceType":"datasetVersion","datasetId":4370376},{"sourceId":7512860,"sourceType":"datasetVersion","datasetId":4375852}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install transformers sentence-transformers datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-18T07:59:45.858824Z","iopub.execute_input":"2024-02-18T07:59:45.859073Z","iopub.status.idle":"2024-02-18T07:59:59.579731Z","shell.execute_reply.started":"2024-02-18T07:59:45.859050Z","shell.execute_reply":"2024-02-18T07:59:59.578596Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.37.0)\nCollecting sentence-transformers\n  Downloading sentence_transformers-2.3.1-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.1)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.1.2)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.4)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.1.99)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (9.5.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->datasets) (2023.12.2)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->sentence-transformers) (1.16.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-2.3.1-py3-none-any.whl (132 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.8/132.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-2.3.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer, models\nfrom transformers import BertTokenizer\nfrom transformers import get_linear_schedule_with_warmup\nimport torch\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport time\nimport datetime\nimport random\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-02-18T07:59:59.581441Z","iopub.execute_input":"2024-02-18T07:59:59.581730Z","iopub.status.idle":"2024-02-18T08:00:06.775879Z","shell.execute_reply.started":"2024-02-18T07:59:59.581703Z","shell.execute_reply":"2024-02-18T08:00:06.775091Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-02-18T08:00:06.777289Z","iopub.execute_input":"2024-02-18T08:00:06.777773Z","iopub.status.idle":"2024-02-18T08:00:06.825242Z","shell.execute_reply.started":"2024-02-18T08:00:06.777747Z","shell.execute_reply":"2024-02-18T08:00:06.824375Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"There are 1 GPU(s) available.\nWe will use the GPU: Tesla P100-PCIE-16GB\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the English version of the STSB dataset\ndataset = load_dataset(\"stsb_multi_mt\", \"en\")\nprint(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-02-18T08:00:06.826935Z","iopub.execute_input":"2024-02-18T08:00:06.827241Z","iopub.status.idle":"2024-02-18T08:00:09.495013Z","shell.execute_reply.started":"2024-02-18T08:00:06.827216Z","shell.execute_reply":"2024-02-18T08:00:09.494144Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.10k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b9726266f9b402ca00bf4724c47c55f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/2.69k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"540875cb655349c78b269c05a0afab89"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset stsb_multi_mt/en (download: 1.02 MiB, generated: 1.06 MiB, post-processed: Unknown size, total: 2.08 MiB) to /root/.cache/huggingface/datasets/stsb_multi_mt/en/1.0.0/a5d260e4b7aa82d1ab7379523a005a366d9b124c76a5a5cf0c4c5365458b0ba9...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"137a2e6d5df24b2a85e3ee2471caa37c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/229k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"559b870b0f984c11a2922d29c0d45a0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/74.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5090045b6c2e4a4fbbb911638a733294"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/52.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e98a98cab1044734a613a1d0794265ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/5749 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1379 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating dev split:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset stsb_multi_mt downloaded and prepared to /root/.cache/huggingface/datasets/stsb_multi_mt/en/1.0.0/a5d260e4b7aa82d1ab7379523a005a366d9b124c76a5a5cf0c4c5365458b0ba9. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"803ac1798a8c45a69f6a7b8bbb726def"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['sentence1', 'sentence2', 'similarity_score'],\n        num_rows: 5749\n    })\n    test: Dataset({\n        features: ['sentence1', 'sentence2', 'similarity_score'],\n        num_rows: 1379\n    })\n    dev: Dataset({\n        features: ['sentence1', 'sentence2', 'similarity_score'],\n        num_rows: 1500\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"len(dataset['train'])","metadata":{"execution":{"iopub.status.busy":"2024-02-18T08:06:22.402977Z","iopub.execute_input":"2024-02-18T08:06:22.403427Z","iopub.status.idle":"2024-02-18T08:06:22.410672Z","shell.execute_reply.started":"2024-02-18T08:06:22.403388Z","shell.execute_reply":"2024-02-18T08:06:22.409524Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"5749"},"metadata":{}}]},{"cell_type":"code","source":"print(\"A sample from the STSB dataset's training split:\")\nfor i in range(10):\n    print(dataset['train'][i])","metadata":{"execution":{"iopub.status.busy":"2024-02-18T08:06:26.046042Z","iopub.execute_input":"2024-02-18T08:06:26.046965Z","iopub.status.idle":"2024-02-18T08:06:26.053862Z","shell.execute_reply.started":"2024-02-18T08:06:26.046928Z","shell.execute_reply":"2024-02-18T08:06:26.052811Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"A sample from the STSB dataset's training split:\n{'sentence1': 'A plane is taking off.', 'sentence2': 'An air plane is taking off.', 'similarity_score': 5.0}\n{'sentence1': 'A man is playing a large flute.', 'sentence2': 'A man is playing a flute.', 'similarity_score': 3.799999952316284}\n{'sentence1': 'A man is spreading shreded cheese on a pizza.', 'sentence2': 'A man is spreading shredded cheese on an uncooked pizza.', 'similarity_score': 3.799999952316284}\n{'sentence1': 'Three men are playing chess.', 'sentence2': 'Two men are playing chess.', 'similarity_score': 2.5999999046325684}\n{'sentence1': 'A man is playing the cello.', 'sentence2': 'A man seated is playing the cello.', 'similarity_score': 4.25}\n{'sentence1': 'Some men are fighting.', 'sentence2': 'Two men are fighting.', 'similarity_score': 4.25}\n{'sentence1': 'A man is smoking.', 'sentence2': 'A man is skating.', 'similarity_score': 0.5}\n{'sentence1': 'The man is playing the piano.', 'sentence2': 'The man is playing the guitar.', 'similarity_score': 1.600000023841858}\n{'sentence1': 'A man is playing on a guitar and singing.', 'sentence2': 'A woman is playing an acoustic guitar and singing.', 'similarity_score': 2.200000047683716}\n{'sentence1': 'A person is throwing a cat on to the ceiling.', 'sentence2': 'A person throws a cat on the ceiling.', 'similarity_score': 5.0}\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/eng-train2/eng_train.csv')","metadata":{"execution":{"iopub.status.busy":"2024-02-18T08:06:29.103545Z","iopub.execute_input":"2024-02-18T08:06:29.104806Z","iopub.status.idle":"2024-02-18T08:06:29.155741Z","shell.execute_reply.started":"2024-02-18T08:06:29.104767Z","shell.execute_reply":"2024-02-18T08:06:29.154788Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-18T08:06:32.448348Z","iopub.execute_input":"2024-02-18T08:06:32.448717Z","iopub.status.idle":"2024-02-18T08:06:32.463596Z","shell.execute_reply.started":"2024-02-18T08:06:32.448686Z","shell.execute_reply":"2024-02-18T08:06:32.462633Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"           PairID                                               Text  Score\n0  ENG-train-0000  It that happens, just pull the plug.\\nif that ...    1.0\n1  ENG-train-0001  A black dog running through water.\\nA black do...    1.0\n2  ENG-train-0002  I've been searchingthe entire abbey for you.\\n...    1.0\n3  ENG-train-0003  If he is good looking and has a good personali...    1.0\n4  ENG-train-0004  She does not hate you, she is just annoyed wit...    1.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PairID</th>\n      <th>Text</th>\n      <th>Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ENG-train-0000</td>\n      <td>It that happens, just pull the plug.\\nif that ...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ENG-train-0001</td>\n      <td>A black dog running through water.\\nA black do...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ENG-train-0002</td>\n      <td>I've been searchingthe entire abbey for you.\\n...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ENG-train-0003</td>\n      <td>If he is good looking and has a good personali...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ENG-train-0004</td>\n      <td>She does not hate you, she is just annoyed wit...</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Split the 'Text' column and create a DataFrame from the resulting list of lists\nsplit_text = df['Text'].apply(lambda x: x.split('\\n', 1)).to_list()\nsplit_text_df = pd.DataFrame(split_text, columns=['sentence1', 'sentence2'])\n\n# Assign the columns from the split_text_df to the original DataFrame (df)\ndf[['sentence1', 'sentence2']] = split_text_df[['sentence1', 'sentence2']]","metadata":{"execution":{"iopub.status.busy":"2024-02-18T08:06:36.400302Z","iopub.execute_input":"2024-02-18T08:06:36.400690Z","iopub.status.idle":"2024-02-18T08:06:36.420013Z","shell.execute_reply.started":"2024-02-18T08:06:36.400660Z","shell.execute_reply":"2024-02-18T08:06:36.419077Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-18T08:06:40.776123Z","iopub.execute_input":"2024-02-18T08:06:40.776473Z","iopub.status.idle":"2024-02-18T08:06:40.787900Z","shell.execute_reply.started":"2024-02-18T08:06:40.776446Z","shell.execute_reply":"2024-02-18T08:06:40.786945Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"           PairID                                               Text  Score  \\\n0  ENG-train-0000  It that happens, just pull the plug.\\nif that ...    1.0   \n1  ENG-train-0001  A black dog running through water.\\nA black do...    1.0   \n2  ENG-train-0002  I've been searchingthe entire abbey for you.\\n...    1.0   \n3  ENG-train-0003  If he is good looking and has a good personali...    1.0   \n4  ENG-train-0004  She does not hate you, she is just annoyed wit...    1.0   \n\n                                           sentence1  \\\n0               It that happens, just pull the plug.   \n1                 A black dog running through water.   \n2       I've been searchingthe entire abbey for you.   \n3  If he is good looking and has a good personali...   \n4  She does not hate you, she is just annoyed wit...   \n\n                                           sentence2  \n0          if that ever happens, just pull the plug.  \n1         A black dog is running through some water.  \n2            I'm looking for you all over the abbey.  \n3  If he's good looking, and a good personality, ...  \n4         She doesn't hate you, she is just annoyed.  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PairID</th>\n      <th>Text</th>\n      <th>Score</th>\n      <th>sentence1</th>\n      <th>sentence2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ENG-train-0000</td>\n      <td>It that happens, just pull the plug.\\nif that ...</td>\n      <td>1.0</td>\n      <td>It that happens, just pull the plug.</td>\n      <td>if that ever happens, just pull the plug.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ENG-train-0001</td>\n      <td>A black dog running through water.\\nA black do...</td>\n      <td>1.0</td>\n      <td>A black dog running through water.</td>\n      <td>A black dog is running through some water.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ENG-train-0002</td>\n      <td>I've been searchingthe entire abbey for you.\\n...</td>\n      <td>1.0</td>\n      <td>I've been searchingthe entire abbey for you.</td>\n      <td>I'm looking for you all over the abbey.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ENG-train-0003</td>\n      <td>If he is good looking and has a good personali...</td>\n      <td>1.0</td>\n      <td>If he is good looking and has a good personali...</td>\n      <td>If he's good looking, and a good personality, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ENG-train-0004</td>\n      <td>She does not hate you, she is just annoyed wit...</td>\n      <td>1.0</td>\n      <td>She does not hate you, she is just annoyed wit...</td>\n      <td>She doesn't hate you, she is just annoyed.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.drop('PairID', axis = 1, inplace = True)\ndf.drop('Text', axis = 1, inplace = True)\ndf['similarity_score'] = df['Score'] * 5.0\ndf.drop('Score', axis=1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-02-18T08:06:44.591308Z","iopub.execute_input":"2024-02-18T08:06:44.591995Z","iopub.status.idle":"2024-02-18T08:06:44.602334Z","shell.execute_reply.started":"2024-02-18T08:06:44.591963Z","shell.execute_reply":"2024-02-18T08:06:44.601434Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df.head()\n#print(len(df))","metadata":{"execution":{"iopub.status.busy":"2024-02-18T08:06:47.294324Z","iopub.execute_input":"2024-02-18T08:06:47.294700Z","iopub.status.idle":"2024-02-18T08:06:47.305165Z","shell.execute_reply.started":"2024-02-18T08:06:47.294672Z","shell.execute_reply":"2024-02-18T08:06:47.304257Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                                           sentence1  \\\n0               It that happens, just pull the plug.   \n1                 A black dog running through water.   \n2       I've been searchingthe entire abbey for you.   \n3  If he is good looking and has a good personali...   \n4  She does not hate you, she is just annoyed wit...   \n\n                                           sentence2  similarity_score  \n0          if that ever happens, just pull the plug.               5.0  \n1         A black dog is running through some water.               5.0  \n2            I'm looking for you all over the abbey.               5.0  \n3  If he's good looking, and a good personality, ...               5.0  \n4         She doesn't hate you, she is just annoyed.               5.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence1</th>\n      <th>sentence2</th>\n      <th>similarity_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>It that happens, just pull the plug.</td>\n      <td>if that ever happens, just pull the plug.</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A black dog running through water.</td>\n      <td>A black dog is running through some water.</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I've been searchingthe entire abbey for you.</td>\n      <td>I'm looking for you all over the abbey.</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>If he is good looking and has a good personali...</td>\n      <td>If he's good looking, and a good personality, ...</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>She does not hate you, she is just annoyed wit...</td>\n      <td>She doesn't hate you, she is just annoyed.</td>\n      <td>5.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"combined  = pd.read_csv('/kaggle/input/combined/combined.csv')","metadata":{"execution":{"iopub.status.busy":"2024-02-18T08:06:52.342281Z","iopub.execute_input":"2024-02-18T08:06:52.342652Z","iopub.status.idle":"2024-02-18T08:06:52.410295Z","shell.execute_reply.started":"2024-02-18T08:06:52.342622Z","shell.execute_reply":"2024-02-18T08:06:52.409559Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"combined.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-18T08:12:06.913354Z","iopub.execute_input":"2024-02-18T08:12:06.914048Z","iopub.status.idle":"2024-02-18T08:12:06.924077Z","shell.execute_reply.started":"2024-02-18T08:12:06.914016Z","shell.execute_reply":"2024-02-18T08:12:06.923173Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"                                           sentence1  \\\n0  The young boys are playing outdoors and the ma...   \n1  A person in a black jacket is doing tricks on ...   \n2       Four children are doing backbends in the gym   \n3                      A player is throwing the ball   \n4  Five children are standing in front of a woode...   \n\n                                           sentence2  similarity_score  \n0  There is no boy playing outdoors and there is ...               3.6  \n1  A skilled person is riding a bicycle on one wheel               3.4  \n2  Four girls are doing backbends and playing out...               3.8  \n3        Two teams are competing in a football match               2.9  \n4         Five children are standing in a wooden hut               4.2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence1</th>\n      <th>sentence2</th>\n      <th>similarity_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The young boys are playing outdoors and the ma...</td>\n      <td>There is no boy playing outdoors and there is ...</td>\n      <td>3.6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A person in a black jacket is doing tricks on ...</td>\n      <td>A skilled person is riding a bicycle on one wheel</td>\n      <td>3.4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Four children are doing backbends in the gym</td>\n      <td>Four girls are doing backbends and playing out...</td>\n      <td>3.8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A player is throwing the ball</td>\n      <td>Two teams are competing in a football match</td>\n      <td>2.9</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Five children are standing in front of a woode...</td>\n      <td>Five children are standing in a wooden hut</td>\n      <td>4.2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"sem = {(row['sentence1'], row['sentence2']): row['similarity_score'] for index, row in df.iterrows()}\ncom = {(row['sentence1'], row['sentence2']): row['similarity_score'] for index, row in combined.iterrows()}\ntr = {(row['sentence1'], row['sentence2']): row['similarity_score'] for row in dataset['train']}","metadata":{"execution":{"iopub.status.busy":"2024-02-18T08:06:56.665579Z","iopub.execute_input":"2024-02-18T08:06:56.665930Z","iopub.status.idle":"2024-02-18T08:06:58.581627Z","shell.execute_reply.started":"2024-02-18T08:06:56.665904Z","shell.execute_reply":"2024-02-18T08:06:58.580807Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(len(sem))\nprint(len(com))\nprint(len(tr))","metadata":{"execution":{"iopub.status.busy":"2024-02-18T08:07:42.683254Z","iopub.execute_input":"2024-02-18T08:07:42.684142Z","iopub.status.idle":"2024-02-18T08:07:42.688966Z","shell.execute_reply.started":"2024-02-18T08:07:42.684109Z","shell.execute_reply":"2024-02-18T08:07:42.687979Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"5500\n16497\n5706\n","output_type":"stream"}]},{"cell_type":"code","source":"result = dict()\ncounter = dict()\n\nfor key, value in list(sem.items())[:]:\n    if key in result:\n        result[key] += value\n        counter[key] += 1\n    else:\n        result[key] = value\n        counter[key] = 1\n        \nfor key, value in list(com.items())[:]:\n    if key in result:\n        result[key] += value\n        counter[key] += 1\n    else:\n        result[key] = value\n        counter[key] = 1\n        \nfor key, value in list(tr.items())[:]:\n    if key in result:\n        result[key] += value\n        counter[key] += 1\n    else:\n        result[key] = value\n        counter[key] = 1\n        \nprint(len(result))","metadata":{"execution":{"iopub.status.busy":"2024-02-18T08:07:43.512998Z","iopub.execute_input":"2024-02-18T08:07:43.513703Z","iopub.status.idle":"2024-02-18T08:07:43.546855Z","shell.execute_reply.started":"2024-02-18T08:07:43.513663Z","shell.execute_reply":"2024-02-18T08:07:43.545961Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"24909\n","output_type":"stream"}]},{"cell_type":"code","source":"for key, value in list(result.items())[:]:\n    result[key] = round(result[key] / (1.00 * counter[key]), 10)","metadata":{"execution":{"iopub.status.busy":"2024-02-18T08:07:46.687161Z","iopub.execute_input":"2024-02-18T08:07:46.687799Z","iopub.status.idle":"2024-02-18T08:07:46.734485Z","shell.execute_reply.started":"2024-02-18T08:07:46.687766Z","shell.execute_reply":"2024-02-18T08:07:46.733696Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"for key, value in list(result.items())[:10]:\n    print(key, value)","metadata":{"execution":{"iopub.status.busy":"2024-02-18T08:07:48.152434Z","iopub.execute_input":"2024-02-18T08:07:48.152792Z","iopub.status.idle":"2024-02-18T08:07:48.160697Z","shell.execute_reply.started":"2024-02-18T08:07:48.152763Z","shell.execute_reply":"2024-02-18T08:07:48.159826Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"('It that happens, just pull the plug.', 'if that ever happens, just pull the plug.') 5.0\n('A black dog running through water.', 'A black dog is running through some water.') 4.8666667302\n(\"I've been searchingthe entire abbey for you.\", \"I'm looking for you all over the abbey.\") 5.0\n('If he is good looking and has a good personality, he might be straight - but is more likely bisexual.', \"If he's good looking, and a good personality, he MIGHT be straight, but more likely bi.\") 5.0\n('She does not hate you, she is just annoyed with you.', \"She doesn't hate you, she is just annoyed.\") 5.0\n('Actor Gazzara dead at 81', 'Actor Ben Gazzara dies at 81') 4.7000000477\n(\"No, I really didn't want New York to win.\", \"No i didn't want New york to win\") 5.0\n('I hae no problems with them.', 'lol i have no problems with them.') 5.0\n('Your parents do not have to like your boyfriend, you do.', 'your parents dont have to like your bf, you do.') 5.0\n('I think Taylor is really cute, but I hate his voice.', 'I think Taylor is SUPER cute...but I hate his voice.') 5.0\n","output_type":"stream"}]},{"cell_type":"code","source":"print(dataset['train'][0]['sentence1'])\n\n\"\"\"baal1 = list(train_df['sentence1'])\nbaal2 = list(train_df['sentence2'])\nbaal3 = list(train_df['similarity_score'])\n\nbaal11 = list(combined['sentence1'])\nbaal22 = list(combined['sentence2'])\nbaal33 = list(combined['similarity_score'])\n\nbaal1.extend(baal11)\nbaal2.extend(baal22)\nbaal3.extend(baal33)\"\"\"\n#updated_dataset = dataset['train'].map(lambda baal : {'sentence1': + baal1['baal']})\n\n#print(len(baal1))\nbaal1 = list(result.keys())\nbaal2 = list(result.values())","metadata":{"execution":{"iopub.status.busy":"2024-02-18T08:07:52.634467Z","iopub.execute_input":"2024-02-18T08:07:52.635359Z","iopub.status.idle":"2024-02-18T08:07:52.642182Z","shell.execute_reply.started":"2024-02-18T08:07:52.635325Z","shell.execute_reply":"2024-02-18T08:07:52.641245Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"A plane is taking off.\n","output_type":"stream"}]},{"cell_type":"code","source":"baal11 = []\nbaal12 = []\n\nfor i in range(len(baal1)):\n    sentence1, sentence2 = baal1[i]\n    baal11.append(sentence1)\n    baal12.append(sentence2)\n\nprint(len(baal11))\nprint(len(baal12))","metadata":{"execution":{"iopub.status.busy":"2024-02-18T08:07:53.879711Z","iopub.execute_input":"2024-02-18T08:07:53.880496Z","iopub.status.idle":"2024-02-18T08:07:53.897794Z","shell.execute_reply.started":"2024-02-18T08:07:53.880463Z","shell.execute_reply":"2024-02-18T08:07:53.896893Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"24909\n24909\n","output_type":"stream"}]},{"cell_type":"code","source":"kf = pd.DataFrame({\n    'sentence1': baal11,\n    'sentence2': baal12,\n    'similarity_score': baal2\n})\n\nkf.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-18T08:08:14.720133Z","iopub.execute_input":"2024-02-18T08:08:14.720519Z","iopub.status.idle":"2024-02-18T08:08:14.741331Z","shell.execute_reply.started":"2024-02-18T08:08:14.720472Z","shell.execute_reply":"2024-02-18T08:08:14.740387Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"                                           sentence1  \\\n0               It that happens, just pull the plug.   \n1                 A black dog running through water.   \n2       I've been searchingthe entire abbey for you.   \n3  If he is good looking and has a good personali...   \n4  She does not hate you, she is just annoyed wit...   \n\n                                           sentence2  similarity_score  \n0          if that ever happens, just pull the plug.          5.000000  \n1         A black dog is running through some water.          4.866667  \n2            I'm looking for you all over the abbey.          5.000000  \n3  If he's good looking, and a good personality, ...          5.000000  \n4         She doesn't hate you, she is just annoyed.          5.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence1</th>\n      <th>sentence2</th>\n      <th>similarity_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>It that happens, just pull the plug.</td>\n      <td>if that ever happens, just pull the plug.</td>\n      <td>5.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A black dog running through water.</td>\n      <td>A black dog is running through some water.</td>\n      <td>4.866667</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I've been searchingthe entire abbey for you.</td>\n      <td>I'm looking for you all over the abbey.</td>\n      <td>5.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>If he is good looking and has a good personali...</td>\n      <td>If he's good looking, and a good personality, ...</td>\n      <td>5.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>She does not hate you, she is just annoyed wit...</td>\n      <td>She doesn't hate you, she is just annoyed.</td>\n      <td>5.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"\"\"\"import random\n\npartition = 3000\n\ndev_baal1 = baal11[21000:]\n#dev_baal1 = random.sample(baal11, partition)\ndev_baal2 = baal12[21000:]\n#dev_baal2 = random.sample(baal12, partition)\ndev_baal3 = baal2[21000:]\n#dev_baal3 = random.sample(baal2, partition)\n\n#baal11 = [element for element in baal11 if element not in dev_baal1]\n\nbaal11 = baal11[:21000]\n\n#baal12 = [element for element in baal12 if element not in dev_baal2]\nbaal12 = baal12[:21000]\n\n#baal2 = [element for element in baal2 if element not in dev_baal3]\nbaal2 = baal2[:21000]\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-01-30T14:15:36.209187Z","iopub.execute_input":"2024-01-30T14:15:36.209538Z","iopub.status.idle":"2024-01-30T14:15:36.216732Z","shell.execute_reply.started":"2024-01-30T14:15:36.209509Z","shell.execute_reply":"2024-01-30T14:15:36.215651Z"},"trusted":true},"execution_count":71,"outputs":[{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"'import random\\n\\npartition = 3000\\n\\ndev_baal1 = baal11[21000:]\\n#dev_baal1 = random.sample(baal11, partition)\\ndev_baal2 = baal12[21000:]\\n#dev_baal2 = random.sample(baal12, partition)\\ndev_baal3 = baal2[21000:]\\n#dev_baal3 = random.sample(baal2, partition)\\n\\n#baal11 = [element for element in baal11 if element not in dev_baal1]\\n\\nbaal11 = baal11[:21000]\\n\\n#baal12 = [element for element in baal12 if element not in dev_baal2]\\nbaal12 = baal12[:21000]\\n\\n#baal2 = [element for element in baal2 if element not in dev_baal3]\\nbaal2 = baal2[:21000]'"},"metadata":{}}]},{"cell_type":"code","source":"\"\"\"print(len(baal11))\nprint(len(dev_baal1))\nfor i in range(10):\n    print(baal11[i], baal12[i], baal2[i])\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-01-30T14:15:36.218268Z","iopub.execute_input":"2024-01-30T14:15:36.219173Z","iopub.status.idle":"2024-01-30T14:15:36.227913Z","shell.execute_reply.started":"2024-01-30T14:15:36.219130Z","shell.execute_reply":"2024-01-30T14:15:36.226823Z"},"trusted":true},"execution_count":72,"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"'print(len(baal11))\\nprint(len(dev_baal1))\\nfor i in range(10):\\n    print(baal11[i], baal12[i], baal2[i])'"},"metadata":{}}]},{"cell_type":"code","source":"\"\"\"print(len(dataset['train']))\nprint(len(baal11))\nprint(len(baal12))\nprint(len(baal2))\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-01-30T14:15:36.229243Z","iopub.execute_input":"2024-01-30T14:15:36.229600Z","iopub.status.idle":"2024-01-30T14:15:36.241089Z","shell.execute_reply.started":"2024-01-30T14:15:36.229559Z","shell.execute_reply":"2024-01-30T14:15:36.240094Z"},"trusted":true},"execution_count":73,"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"\"print(len(dataset['train']))\\nprint(len(baal11))\\nprint(len(baal12))\\nprint(len(baal2))\""},"metadata":{}}]},{"cell_type":"code","source":"\"\"\"from datasets import DatasetDict, Dataset\n\ndataset222 = dataset.copy()\n#print(dataset222)\n\nupdated_train_dataa = []\nupdated_train_dataa = [{'sentence1': s1, 'sentence2': s2, 'similarity_score': score} for s1, s2, score in zip(baal11, baal12, baal2)]\n\ndataset222['train'] = updated_train_dataa\n\nprint(len(dataset222['train']))\nprint(len(updated_train_dataa))\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-01-30T14:15:36.242604Z","iopub.execute_input":"2024-01-30T14:15:36.243426Z","iopub.status.idle":"2024-01-30T14:15:36.254941Z","shell.execute_reply.started":"2024-01-30T14:15:36.243386Z","shell.execute_reply":"2024-01-30T14:15:36.253828Z"},"trusted":true},"execution_count":74,"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"\"from datasets import DatasetDict, Dataset\\n\\ndataset222 = dataset.copy()\\n#print(dataset222)\\n\\nupdated_train_dataa = []\\nupdated_train_dataa = [{'sentence1': s1, 'sentence2': s2, 'similarity_score': score} for s1, s2, score in zip(baal11, baal12, baal2)]\\n\\ndataset222['train'] = updated_train_dataa\\n\\nprint(len(dataset222['train']))\\nprint(len(updated_train_dataa))\""},"metadata":{}}]},{"cell_type":"code","source":"\"\"\"for i in range(10):\n    print(dataset222['test'][i])\nprint(len(dataset222['train']))\nprint(len(dataset222['test']))\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-01-30T14:15:36.256401Z","iopub.execute_input":"2024-01-30T14:15:36.256754Z","iopub.status.idle":"2024-01-30T14:15:36.265884Z","shell.execute_reply.started":"2024-01-30T14:15:36.256721Z","shell.execute_reply":"2024-01-30T14:15:36.264878Z"},"trusted":true},"execution_count":75,"outputs":[{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"\"for i in range(10):\\n    print(dataset222['test'][i])\\nprint(len(dataset222['train']))\\nprint(len(dataset222['test']))\""},"metadata":{}}]},{"cell_type":"code","source":"\"\"\"from datasets import DatasetDict, Dataset\n\ndataset['train'][0]['sentence1']\n\n#baal1 = list(train_df['sentence1'])\n#baal2 = list(train_df['sentence2'])\n#baal3 = list(train_df['similarity_score'])\n\n#updated_train_data\ndataset2 = dataset.copy()\n#print(len(dataset['train']))\n# Create a new list of dictionaries with updated values\nupdated_train_data = []\n#print(len(updated_train_data))\n\nupdated_train_data = [{'sentence1': s1, 'sentence2': s2, 'similarity_score': score} for s1, s2, score in zip(baal11, baal12, baal2)]\n\n#updated_train_data1 = [dataset['train'][i] for i in range(len(dataset['train']))]\n#print(len(updated_train_data1))\n\n#updated_train_data1.extend(updated_train_data)\n# Assign the updated list back to dataset['train'] \n#dataset2['train'] = updated_train_data\n#print(len(dataset2['train']))\n#print(len(updated_train_data))\n\n#for i in range(0, 5749):\n#    dataset2['train'][i] = dataset['train'][i].copy()\n\ndataset = dataset222.copy()\n\nprint(len(dataset['train']))\nprint(len(dataset['test']))\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-01-30T14:15:36.267265Z","iopub.execute_input":"2024-01-30T14:15:36.267895Z","iopub.status.idle":"2024-01-30T14:15:36.282195Z","shell.execute_reply.started":"2024-01-30T14:15:36.267865Z","shell.execute_reply":"2024-01-30T14:15:36.281269Z"},"trusted":true},"execution_count":76,"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"\"from datasets import DatasetDict, Dataset\\n\\ndataset['train'][0]['sentence1']\\n\\n#baal1 = list(train_df['sentence1'])\\n#baal2 = list(train_df['sentence2'])\\n#baal3 = list(train_df['similarity_score'])\\n\\n#updated_train_data\\ndataset2 = dataset.copy()\\n#print(len(dataset['train']))\\n# Create a new list of dictionaries with updated values\\nupdated_train_data = []\\n#print(len(updated_train_data))\\n\\nupdated_train_data = [{'sentence1': s1, 'sentence2': s2, 'similarity_score': score} for s1, s2, score in zip(baal11, baal12, baal2)]\\n\\n#updated_train_data1 = [dataset['train'][i] for i in range(len(dataset['train']))]\\n#print(len(updated_train_data1))\\n\\n#updated_train_data1.extend(updated_train_data)\\n# Assign the updated list back to dataset['train'] \\n#dataset2['train'] = updated_train_data\\n#print(len(dataset2['train']))\\n#print(len(updated_train_data))\\n\\n#for i in range(0, 5749):\\n#    dataset2['train'][i] = dataset['train'][i].copy()\\n\\ndataset = dataset222.copy()\\n\\nprint(len(dataset['train']))\\nprint(len(dataset['test']))\""},"metadata":{}}]},{"cell_type":"code","source":"#len(dataset222['train'])","metadata":{"execution":{"iopub.status.busy":"2024-01-30T14:15:36.283666Z","iopub.execute_input":"2024-01-30T14:15:36.283980Z","iopub.status.idle":"2024-01-30T14:15:36.292111Z","shell.execute_reply.started":"2024-01-30T14:15:36.283954Z","shell.execute_reply":"2024-01-30T14:15:36.291130Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"#print(len(dataset['dev']))","metadata":{"execution":{"iopub.status.busy":"2024-01-30T14:15:36.293406Z","iopub.execute_input":"2024-01-30T14:15:36.293792Z","iopub.status.idle":"2024-01-30T14:15:36.303761Z","shell.execute_reply.started":"2024-01-30T14:15:36.293757Z","shell.execute_reply":"2024-01-30T14:15:36.302893Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"sf=pd.read_csv('/kaggle/input/ansdata/eng_dev_with_labels.csv')\n\nsplit_text = sf['Text'].apply(lambda x: x.split('\\n', 1)).to_list()\nsplit_text_sf = pd.DataFrame(split_text, columns=['sentence1', 'sentence2'])\n\n# Assign the columns from the split_text_df to the original DataFrame (df)\nsf[['sentence1', 'sentence2']] = split_text_df[['sentence1', 'sentence2']]\n\nsf.drop('PairID', axis = 1, inplace = True)\nsf.drop('Text', axis = 1, inplace = True)\n\nsf['similarity_score'] = sf['Score'] * 5.0\nsf.drop('Score', axis=1, inplace = True)\n\nsf.head()\n#sf.shape\nprint(sf.shape)\nprint(kf.shape)","metadata":{"execution":{"iopub.status.busy":"2024-02-18T08:08:24.320693Z","iopub.execute_input":"2024-02-18T08:08:24.321051Z","iopub.status.idle":"2024-02-18T08:08:24.338325Z","shell.execute_reply.started":"2024-02-18T08:08:24.321023Z","shell.execute_reply":"2024-02-18T08:08:24.337492Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"(250, 3)\n(24909, 3)\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_com_df, dev_df = train_test_split(kf, test_size = 0.2, random_state = 1)","metadata":{"execution":{"iopub.status.busy":"2024-02-18T08:08:35.252477Z","iopub.execute_input":"2024-02-18T08:08:35.253384Z","iopub.status.idle":"2024-02-18T08:08:35.262921Z","shell.execute_reply.started":"2024-02-18T08:08:35.253352Z","shell.execute_reply":"2024-02-18T08:08:35.261919Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"print(len(train_com_df))\nprint(len(dev_df))","metadata":{"execution":{"iopub.status.busy":"2024-02-18T08:10:04.725917Z","iopub.execute_input":"2024-02-18T08:10:04.726320Z","iopub.status.idle":"2024-02-18T08:10:04.732020Z","shell.execute_reply.started":"2024-02-18T08:10:04.726289Z","shell.execute_reply":"2024-02-18T08:10:04.730866Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"19927\n4982\n","output_type":"stream"}]},{"cell_type":"code","source":"kf=[dev_df,sf]\n#kf = sf\ndev_df = pd.concat(kf)\ndev_df.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-30T14:15:36.356619Z","iopub.execute_input":"2024-01-30T14:15:36.357152Z","iopub.status.idle":"2024-01-30T14:15:36.370185Z","shell.execute_reply.started":"2024-01-30T14:15:36.357112Z","shell.execute_reply":"2024-01-30T14:15:36.368955Z"},"trusted":true},"execution_count":82,"outputs":[{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"(5232, 3)"},"metadata":{}}]},{"cell_type":"code","source":"dev_com_df = dev_df","metadata":{"execution":{"iopub.status.busy":"2024-01-30T14:15:36.371767Z","iopub.execute_input":"2024-01-30T14:15:36.372171Z","iopub.status.idle":"2024-01-30T14:15:36.386682Z","shell.execute_reply.started":"2024-01-30T14:15:36.372138Z","shell.execute_reply":"2024-01-30T14:15:36.385582Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"print(train_com_df.shape)\nprint(dev_com_df.shape)","metadata":{"execution":{"iopub.status.busy":"2024-01-30T14:15:36.388149Z","iopub.execute_input":"2024-01-30T14:15:36.388542Z","iopub.status.idle":"2024-01-30T14:15:36.399397Z","shell.execute_reply.started":"2024-01-30T14:15:36.388511Z","shell.execute_reply":"2024-01-30T14:15:36.398137Z"},"trusted":true},"execution_count":84,"outputs":[{"name":"stdout","text":"(19927, 3)\n(5232, 3)\n","output_type":"stream"}]},{"cell_type":"code","source":"print(dataset['train'][0]['sentence1'])\n\n#baal1 = list(train_df['sentence1'])\n#baal2 = list(train_df['sentence2'])\n#baal3 = list(train_df['similarity_score'])\n\nbaal1 = list(train_com_df['sentence1'])\nbaal2 = list(train_com_df['sentence2'])\nbaal3 = list(train_com_df['similarity_score'])\n\n#baal1.extend(baal11)\n##baal2.extend(baal22)\n#baal3.extend(baal33)\n#updated_dataset = dataset['train'].map(lambda baal : {'sentence1': + baal1['baal']})\n\nprint(len(baal1))","metadata":{"execution":{"iopub.status.busy":"2024-01-30T14:15:36.400870Z","iopub.execute_input":"2024-01-30T14:15:36.401327Z","iopub.status.idle":"2024-01-30T14:15:36.426712Z","shell.execute_reply.started":"2024-01-30T14:15:36.401279Z","shell.execute_reply":"2024-01-30T14:15:36.425475Z"},"trusted":true},"execution_count":85,"outputs":[{"name":"stdout","text":"A plane is taking off.\n19927\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import DatasetDict, Dataset\n\ndataset['train'][0]['sentence1']\n\n#baal1 = list(train_df['sentence1'])\n#baal2 = list(train_df['sentence2'])\n#baal3 = list(train_df['similarity_score'])\n\n#updated_train_data\ndataset2 = dataset.copy()\n#print(len(dataset['train']))\n# Create a new list of dictionaries with updated values\nupdated_train_data = []\n#print(len(updated_train_data))\n\nupdated_train_data = [{'sentence1': s1, 'sentence2': s2, 'similarity_score': score} for s1, s2, score in zip(baal1, baal2, baal3)]\n\nupdated_train_data1 = [dataset['train'][i] for i in range(len(dataset['train']))]\nprint(len(updated_train_data1))\n\nupdated_train_data1.extend(updated_train_data)\n# Assign the updated list back to dataset['train']\n\ndataset2['train'] = updated_train_data1\nprint(len(dataset2['train']))\n#print(len(updated_train_data))\n\n#for i in range(0, 5749):\n#    dataset2['train'][i] = dataset['train'][i].copy()\n\ndataset['train'] = dataset2['train'].copy()\n\nprint(len(dataset['train']))","metadata":{"execution":{"iopub.status.busy":"2024-01-30T14:15:36.428358Z","iopub.execute_input":"2024-01-30T14:15:36.428795Z","iopub.status.idle":"2024-01-30T14:15:36.874333Z","shell.execute_reply.started":"2024-01-30T14:15:36.428733Z","shell.execute_reply":"2024-01-30T14:15:36.873331Z"},"trusted":true},"execution_count":86,"outputs":[{"name":"stdout","text":"5749\n25676\n25676\n","output_type":"stream"}]},{"cell_type":"code","source":"#baal1 = list(dev_df['sentence1'])\n#baal2 = list(dev_df['sentence2'])\n#baal3 = list(dev_df['similarity_score'])\n\nbaal1 = list(dev_com_df['sentence1'])\nbaal2 = list(dev_com_df['sentence2'])\nbaal3 = list(dev_com_df['similarity_score'])\n\n#baal1.extend(baal11)\n#baal2.extend(baal22)\n#baal3.extend(baal33)\n\n\n#print(len(baal1))\nupdated_dev_data = []\n# Create a new list of dictionaries with updated values\nupdated_dev_data = [{'sentence1': s1, 'sentence2': s2, 'similarity_score': score} for s1, s2, score in zip(baal1, baal2, baal3)]\nupdated_dev_data1 = [dataset['dev'][i] for i in range(len(dataset['dev']))]\n\n#print(len(updated_dev_data))\n#print(len(updated_dev_data1))\n\nupdated_dev_data1.extend(updated_dev_data)\nprint(len(updated_dev_data1))\n# Assign the updated list back to dataset['train']\n\ndataset2['dev'] = updated_dev_data1\n#print(len(updated_dev_data))\n#print(len(dataset['dev']))\n#for i in range(0,1500):\n#    dataset2['dev'][i] = dataset['dev'][i].copy()\n\ndataset['dev'] = dataset2['dev'].copy()\n    \nprint(len(dataset['dev']))","metadata":{"execution":{"iopub.status.busy":"2024-01-30T14:15:36.875601Z","iopub.execute_input":"2024-01-30T14:15:36.875886Z","iopub.status.idle":"2024-01-30T14:15:37.007283Z","shell.execute_reply.started":"2024-01-30T14:15:36.875862Z","shell.execute_reply":"2024-01-30T14:15:37.006216Z"},"trusted":true},"execution_count":87,"outputs":[{"name":"stdout","text":"6732\n6732\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import RobertaTokenizer\n\n# Use the correct tokenizer class for RoBERTa\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","metadata":{"execution":{"iopub.status.busy":"2024-01-30T14:15:37.009293Z","iopub.execute_input":"2024-01-30T14:15:37.010200Z","iopub.status.idle":"2024-01-30T14:15:37.430475Z","shell.execute_reply.started":"2024-01-30T14:15:37.010139Z","shell.execute_reply":"2024-01-30T14:15:37.429306Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"class STSBDataset(torch.utils.data.Dataset):\n\n    def __init__(self, dataset):\n        # Normalize the similarity scores in the dataset\n        similarity_scores = [i['similarity_score'] for i in dataset]\n        self.normalized_similarity_scores = [i/5.0 for i in similarity_scores]\n        self.first_sentences = [i['sentence1'] for i in dataset]\n        self.second_sentences = [i['sentence2'] for i in dataset]\n        self.concatenated_sentences = [[str(x), str(y)] for x,y in   zip(self.first_sentences, self.second_sentences)]\n\n    def __len__(self):\n        return len(self.concatenated_sentences)\n\n    def get_batch_labels(self, idx):\n        return torch.tensor(self.normalized_similarity_scores[idx])\n\n    def get_batch_texts(self, idx):\n        return tokenizer(self.concatenated_sentences[idx], padding='max_length', max_length=128, truncation=True, return_tensors=\"pt\")\n\n    def __getitem__(self, idx):\n        batch_texts = self.get_batch_texts(idx)\n        batch_y = self.get_batch_labels(idx)\n        return batch_texts, batch_y\n\n\n\ndef collate_fn(texts):\n    input_ids = texts['input_ids']\n    attention_masks = texts['attention_mask']\n    features = [{'input_ids': input_id, 'attention_mask': attention_mask}\n                for input_id, attention_mask in zip(input_ids, attention_masks)]\n    return features","metadata":{"execution":{"iopub.status.busy":"2024-01-30T14:15:37.431909Z","iopub.execute_input":"2024-01-30T14:15:37.432381Z","iopub.status.idle":"2024-01-30T14:15:37.443857Z","shell.execute_reply.started":"2024-01-30T14:15:37.432316Z","shell.execute_reply":"2024-01-30T14:15:37.442645Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"class BertForSTS(torch.nn.Module):\n\n    def __init__(self):\n        super(BertForSTS, self).__init__()\n        self.bert = models.Transformer('RoBERTa-base', max_seq_length=512)\n        self.pooling_layer = models.Pooling(self.bert.get_word_embedding_dimension())\n        self.sts_bert = SentenceTransformer(modules=[self.bert, self.pooling_layer])\n\n    def forward(self, input_data):\n        output = self.sts_bert(input_data)['sentence_embedding']\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-01-30T14:15:37.452169Z","iopub.execute_input":"2024-01-30T14:15:37.453426Z","iopub.status.idle":"2024-01-30T14:15:37.461881Z","shell.execute_reply.started":"2024-01-30T14:15:37.453332Z","shell.execute_reply":"2024-01-30T14:15:37.460768Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"# Instantiate the model and move it to GPU\nmodel = BertForSTS()\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-01-30T14:15:37.463296Z","iopub.execute_input":"2024-01-30T14:15:37.463644Z","iopub.status.idle":"2024-01-30T14:15:39.121148Z","shell.execute_reply.started":"2024-01-30T14:15:37.463614Z","shell.execute_reply":"2024-01-30T14:15:39.120152Z"},"trusted":true},"execution_count":91,"outputs":[{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at RoBERTa-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":91,"output_type":"execute_result","data":{"text/plain":"BertForSTS(\n  (bert): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: RobertaModel \n  (pooling_layer): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False})\n  (sts_bert): SentenceTransformer(\n    (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: RobertaModel \n    (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False})\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"class CosineSimilarityLoss(torch.nn.Module):\n\n    def __init__(self,  loss_fn=torch.nn.MSELoss(), transform_fn=torch.nn.Identity()):\n        super(CosineSimilarityLoss, self).__init__()\n        self.loss_fn = loss_fn\n        self.transform_fn = transform_fn\n        self.cos_similarity = torch.nn.CosineSimilarity(dim=1)\n\n    def forward(self, inputs, labels):\n        emb_1 = torch.stack([inp[0] for inp in inputs])\n        emb_2 = torch.stack([inp[1] for inp in inputs])\n        outputs = self.transform_fn(self.cos_similarity(emb_1, emb_2))\n        return self.loss_fn(outputs, labels.squeeze())","metadata":{"execution":{"iopub.status.busy":"2024-01-30T14:15:39.122534Z","iopub.execute_input":"2024-01-30T14:15:39.122908Z","iopub.status.idle":"2024-01-30T14:15:39.131396Z","shell.execute_reply.started":"2024-01-30T14:15:39.122874Z","shell.execute_reply":"2024-01-30T14:15:39.130309Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"train_ds = STSBDataset(dataset['train'])\nval_ds = STSBDataset(dataset['dev'])\n\n# Create a 90-10 train-validation split.\ntrain_size = len(train_ds)\nval_size = len(val_ds)\n\nprint('{:>5,} training samples'.format(train_size))\nprint('{:>5,} validation samples'.format(val_size))","metadata":{"execution":{"iopub.status.busy":"2024-01-30T14:15:39.132813Z","iopub.execute_input":"2024-01-30T14:15:39.133187Z","iopub.status.idle":"2024-01-30T14:15:39.179630Z","shell.execute_reply.started":"2024-01-30T14:15:39.133149Z","shell.execute_reply":"2024-01-30T14:15:39.178543Z"},"trusted":true},"execution_count":93,"outputs":[{"name":"stdout","text":"25,676 training samples\n6,732 validation samples\n","output_type":"stream"}]},{"cell_type":"code","source":"train_ds[0]","metadata":{"execution":{"iopub.status.busy":"2024-01-30T14:15:39.180998Z","iopub.execute_input":"2024-01-30T14:15:39.181347Z","iopub.status.idle":"2024-01-30T14:15:39.193743Z","shell.execute_reply.started":"2024-01-30T14:15:39.181319Z","shell.execute_reply":"2024-01-30T14:15:39.192641Z"},"trusted":true},"execution_count":94,"outputs":[{"execution_count":94,"output_type":"execute_result","data":{"text/plain":"({'input_ids': tensor([[   0,  250, 3286,   16,  602,  160,    4,    2,    1,    1,    1,    1,\n             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n             1,    1,    1,    1,    1,    1,    1,    1],\n         [   0, 4688,  935, 3286,   16,  602,  160,    4,    2,    1,    1,    1,\n             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n             1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n             1,    1,    1,    1,    1,    1,    1,    1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0],\n         [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0]])},\n tensor(1.))"},"metadata":{}}]},{"cell_type":"code","source":"batch_size = 8\n\ntrain_dataloader = DataLoader(\n            train_ds,  # The training samples.\n            num_workers = 4,\n            batch_size = batch_size, # Use this batch size.\n            shuffle=True # Select samples randomly for each batch\n        )\n\nvalidation_dataloader = DataLoader(\n            val_ds,\n            num_workers = 4,\n            batch_size = batch_size # Use the same batch size\n        )","metadata":{"execution":{"iopub.status.busy":"2024-01-30T14:15:39.196936Z","iopub.execute_input":"2024-01-30T14:15:39.197288Z","iopub.status.idle":"2024-01-30T14:15:39.209384Z","shell.execute_reply.started":"2024-01-30T14:15:39.197255Z","shell.execute_reply":"2024-01-30T14:15:39.208219Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(),\n                  lr = 1e-6)\nepochs = 12\n# Total number of training steps is [number of batches] x [number of epochs].\ntotal_steps = len(train_dataloader) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer,\n                                            num_warmup_steps = 0,\n                                            num_training_steps = total_steps)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-30T14:15:39.210651Z","iopub.execute_input":"2024-01-30T14:15:39.211000Z","iopub.status.idle":"2024-01-30T14:15:39.222910Z","shell.execute_reply.started":"2024-01-30T14:15:39.210974Z","shell.execute_reply":"2024-01-30T14:15:39.222078Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"# Define format_time function\nimport time\n\ndef format_time(elapsed):\n    \"\"\"\n    Takes a time in seconds and returns a string hh:mm:ss\n    \"\"\"\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_rounded)))","metadata":{"execution":{"iopub.status.busy":"2024-01-30T14:15:39.224597Z","iopub.execute_input":"2024-01-30T14:15:39.224928Z","iopub.status.idle":"2024-01-30T14:15:39.231362Z","shell.execute_reply.started":"2024-01-30T14:15:39.224903Z","shell.execute_reply":"2024-01-30T14:15:39.230441Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"def train():\n  seed_val = 42\n  criterion = CosineSimilarityLoss()\n  criterion = criterion.cuda()\n  random.seed(seed_val)\n  torch.manual_seed(seed_val)\n  # We'll store a number of quantities such as training and validation loss,\n  # validation accuracy, and timings.\n  training_stats = []\n  total_t0 = time.time()\n  for epoch_i in range(0, epochs):\n      t0 = time.time()\n      total_train_loss = 0\n      model.train()\n      # For each batch of training data...\n      for train_data, train_label in tqdm(train_dataloader):\n          train_data['input_ids'] = train_data['input_ids'].to(device)\n          train_data['attention_mask'] = train_data['attention_mask'].to(device)\n          train_data = collate_fn(train_data)\n          model.zero_grad()\n          output = [model(feature) for feature in train_data]\n          loss = criterion(output, train_label.to(device))\n          total_train_loss += loss.item()\n          loss.backward()\n          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n          optimizer.step()\n          scheduler.step()\n\n      # Calculate the average loss over all of the batches.\n      avg_train_loss = total_train_loss / len(train_dataloader)\n      # Measure how long this epoch took.\n      training_time = format_time(time.time() - t0)\n      t0 = time.time()\n      model.eval()\n      total_eval_accuracy = 0\n      total_eval_loss = 0\n      nb_eval_steps = 0\n      # Evaluate data for one epoch\n      for val_data, val_label in tqdm(validation_dataloader):\n          val_data['input_ids'] = val_data['input_ids'].to(device)\n          val_data['attention_mask'] = val_data['attention_mask'].to(device)\n          val_data = collate_fn(val_data)\n          with torch.no_grad():\n              output = [model(feature) for feature in val_data]\n          loss = criterion(output, val_label.to(device))\n          total_eval_loss += loss.item()\n      # Calculate the average loss over all of the batches.\n      avg_val_loss = total_eval_loss / len(validation_dataloader)\n      # Measure how long the validation run took.\n      validation_time = format_time(time.time() - t0)\n      # Record all statistics from this epoch.\n      training_stats.append(\n          {\n              'epoch': epoch_i + 1,\n              'Training Loss': avg_train_loss,\n              'Valid. Loss': avg_val_loss,\n              'Training Time': training_time,\n              'Validation Time': validation_time\n          }\n      )\n  return model, training_stats\n\n# Launch the training\nmodel, training_stats = train()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-30T14:15:39.232554Z","iopub.execute_input":"2024-01-30T14:15:39.232837Z","iopub.status.idle":"2024-01-30T18:04:29.370055Z","shell.execute_reply.started":"2024-01-30T14:15:39.232812Z","shell.execute_reply":"2024-01-30T18:04:29.368679Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":98,"outputs":[{"name":"stderr","text":"100%|██████████| 3210/3210 [17:40<00:00,  3.03it/s]\n100%|██████████| 842/842 [01:26<00:00,  9.76it/s]\n100%|██████████| 3210/3210 [17:39<00:00,  3.03it/s]\n100%|██████████| 842/842 [01:25<00:00,  9.79it/s]\n100%|██████████| 3210/3210 [17:31<00:00,  3.05it/s]\n100%|██████████| 842/842 [01:25<00:00,  9.84it/s]\n100%|██████████| 3210/3210 [17:30<00:00,  3.06it/s]\n100%|██████████| 842/842 [01:25<00:00,  9.81it/s]\n100%|██████████| 3210/3210 [17:34<00:00,  3.04it/s]\n100%|██████████| 842/842 [01:26<00:00,  9.77it/s]\n100%|██████████| 3210/3210 [17:41<00:00,  3.02it/s]\n100%|██████████| 842/842 [01:25<00:00,  9.83it/s]\n100%|██████████| 3210/3210 [17:37<00:00,  3.03it/s]\n100%|██████████| 842/842 [01:26<00:00,  9.76it/s]\n100%|██████████| 3210/3210 [17:38<00:00,  3.03it/s]\n100%|██████████| 842/842 [01:26<00:00,  9.76it/s]\n100%|██████████| 3210/3210 [17:41<00:00,  3.02it/s]\n100%|██████████| 842/842 [01:25<00:00,  9.81it/s]\n100%|██████████| 3210/3210 [17:39<00:00,  3.03it/s]\n100%|██████████| 842/842 [01:26<00:00,  9.73it/s]\n100%|██████████| 3210/3210 [17:40<00:00,  3.03it/s]\n100%|██████████| 842/842 [01:26<00:00,  9.74it/s]\n100%|██████████| 3210/3210 [17:40<00:00,  3.03it/s]\n100%|██████████| 842/842 [01:26<00:00,  9.74it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create a DataFrame from our training statistics\ndf_stats = pd.DataFrame(data=training_stats)\n\n# Use the 'epoch' as the row index\ndf_stats = df_stats.set_index('epoch')\n\n# Display the table\ndf_stats","metadata":{"execution":{"iopub.status.busy":"2024-01-30T18:04:29.373632Z","iopub.execute_input":"2024-01-30T18:04:29.373954Z","iopub.status.idle":"2024-01-30T18:04:29.394514Z","shell.execute_reply.started":"2024-01-30T18:04:29.373924Z","shell.execute_reply":"2024-01-30T18:04:29.393369Z"},"trusted":true},"execution_count":99,"outputs":[{"execution_count":99,"output_type":"execute_result","data":{"text/plain":"       Training Loss  Valid. Loss Training Time Validation Time\nepoch                                                          \n1           0.038794     0.030792      00:17:40        00:01:26\n2           0.021645     0.026976      00:17:40        00:01:26\n3           0.018398     0.025044      00:17:31        00:01:26\n4           0.016545     0.023949      00:17:31        00:01:26\n5           0.015097     0.023466      00:17:35        00:01:26\n6           0.014061     0.023109      00:17:41        00:01:26\n7           0.013276     0.022480      00:17:38        00:01:26\n8           0.012533     0.022392      00:17:39        00:01:26\n9           0.012047     0.022405      00:17:41        00:01:26\n10          0.011619     0.022030      00:17:40        00:01:27\n11          0.011344     0.021935      00:17:40        00:01:26\n12          0.011204     0.021921      00:17:41        00:01:26","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Training Loss</th>\n      <th>Valid. Loss</th>\n      <th>Training Time</th>\n      <th>Validation Time</th>\n    </tr>\n    <tr>\n      <th>epoch</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.038794</td>\n      <td>0.030792</td>\n      <td>00:17:40</td>\n      <td>00:01:26</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.021645</td>\n      <td>0.026976</td>\n      <td>00:17:40</td>\n      <td>00:01:26</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.018398</td>\n      <td>0.025044</td>\n      <td>00:17:31</td>\n      <td>00:01:26</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.016545</td>\n      <td>0.023949</td>\n      <td>00:17:31</td>\n      <td>00:01:26</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.015097</td>\n      <td>0.023466</td>\n      <td>00:17:35</td>\n      <td>00:01:26</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.014061</td>\n      <td>0.023109</td>\n      <td>00:17:41</td>\n      <td>00:01:26</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.013276</td>\n      <td>0.022480</td>\n      <td>00:17:38</td>\n      <td>00:01:26</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.012533</td>\n      <td>0.022392</td>\n      <td>00:17:39</td>\n      <td>00:01:26</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.012047</td>\n      <td>0.022405</td>\n      <td>00:17:41</td>\n      <td>00:01:26</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.011619</td>\n      <td>0.022030</td>\n      <td>00:17:40</td>\n      <td>00:01:27</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.011344</td>\n      <td>0.021935</td>\n      <td>00:17:40</td>\n      <td>00:01:26</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.011204</td>\n      <td>0.021921</td>\n      <td>00:17:41</td>\n      <td>00:01:26</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# load the test set\ntest_dataset = load_dataset(\"stsb_multi_mt\", name=\"en\", split=\"test\")\n\n\ntest_df = pd.read_csv('/kaggle/input/test-data/eng_test.csv')\n#print(test_dataset)\n\ntest_df[['sentence1', 'sentence2']] = test_df['Text'].apply(lambda x: x.split('\\n', 1)).to_list()\ntest_df.drop('Text', axis=1, inplace=True)\n#dev_df.drop('PairID', axis=1, inplace=True)\ntest_df['similarity_score'] = 0.0\nbaal1 = list(test_df['sentence1'])\nbaal2 = list(test_df['sentence2'])\nbaal3 = list(test_df['similarity_score'])\n# Create a new list of dictionaries with updated values\nupdated_train_data = [{'sentence1': s1, 'sentence2': s2, 'similarity_score': s3} for s1, s2, s3 in zip(baal1, baal2, baal3)]\n\n# Assign the updated list back to dataset['train']\ntest_dataset = updated_train_data\n\n\n# Prepare the data\nfirst_sent = [i['sentence1'] for i in test_dataset]\nsecond_sent = [i['sentence2'] for i in test_dataset]\nfull_text = [[str(x), str(y)] for x,y in zip(first_sent, second_sent)]\n\nprint(len(full_text))\n\nmodel.eval()\n\n\"\"\"def predict_similarity(sentence_pair):\n  test_input = tokenizer(sentence_pair, padding='max_length', max_length = 128, truncation=True, return_tensors=\"pt\").to(device)\n  test_input['input_ids'] = test_input['input_ids']\n  test_input['attention_mask'] = test_input['attention_mask']\n\n  del test_input['token_type_ids']\n  output = model(test_input)\n  sim = torch.nn.functional.cosine_similarity(output[0], output[1], dim=0).item()\n  return sim\"\"\"\n\ndef predict_similarity(sentence_pair):\n    test_input = tokenizer(sentence_pair, padding='max_length', max_length=128, truncation=True, return_tensors=\"pt\").to(device)\n    \n    # Ensure 'token_type_ids' is not present for RoBERTa\n    if 'token_type_ids' in test_input:\n        del test_input['token_type_ids']\n\n    output = model(test_input)\n    sim = torch.nn.functional.cosine_similarity(output[0], output[1], dim=0).item()\n    return sim","metadata":{"execution":{"iopub.status.busy":"2024-01-30T18:19:19.189079Z","iopub.execute_input":"2024-01-30T18:19:19.190100Z","iopub.status.idle":"2024-01-30T18:19:19.880475Z","shell.execute_reply.started":"2024-01-30T18:19:19.190063Z","shell.execute_reply":"2024-01-30T18:19:19.879311Z"},"trusted":true},"execution_count":126,"outputs":[{"name":"stdout","text":"2600\n","output_type":"stream"}]},{"cell_type":"code","source":"example_1 = full_text[0]\nprint(f\"Sentence 1: {example_1[0]}\")\nprint(f\"Sentence 2: {example_1[1]}\")\nprint(f\"Predicted similarity score: {round(predict_similarity(example_1), 2)}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-30T18:19:23.475484Z","iopub.execute_input":"2024-01-30T18:19:23.475857Z","iopub.status.idle":"2024-01-30T18:19:23.500157Z","shell.execute_reply.started":"2024-01-30T18:19:23.475830Z","shell.execute_reply":"2024-01-30T18:19:23.499121Z"},"trusted":true},"execution_count":127,"outputs":[{"name":"stdout","text":"Sentence 1: Egypt's Brotherhood stands ground after killings\nSentence 2: Egypt: Muslim Brotherhood Stands Behind Morsi\nPredicted similarity score: 0.64\n","output_type":"stream"}]},{"cell_type":"code","source":"scor_list = []\nfor i in full_text:\n    score = round(predict_similarity(i), 2)\n    if(score<0):\n        score = 0.00\n#     print(score)\n    scor_list.append(score)","metadata":{"execution":{"iopub.status.busy":"2024-01-30T18:19:27.068791Z","iopub.execute_input":"2024-01-30T18:19:27.069571Z","iopub.status.idle":"2024-01-30T18:20:09.432252Z","shell.execute_reply.started":"2024-01-30T18:19:27.069539Z","shell.execute_reply":"2024-01-30T18:20:09.431085Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"test_df.drop('sentence1', axis=1, inplace=True)\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-30T18:21:07.770896Z","iopub.execute_input":"2024-01-30T18:21:07.771326Z","iopub.status.idle":"2024-01-30T18:21:07.785909Z","shell.execute_reply.started":"2024-01-30T18:21:07.771293Z","shell.execute_reply":"2024-01-30T18:21:07.784751Z"},"trusted":true},"execution_count":129,"outputs":[{"execution_count":129,"output_type":"execute_result","data":{"text/plain":"          PairID                                          sentence2  \\\n0  ENG-test-0000      Egypt: Muslim Brotherhood Stands Behind Morsi   \n1  ENG-test-0001  Install the program, which is free to download...   \n2  ENG-test-0002  Pretty much the first thing people mentioned w...   \n3  ENG-test-0003    You can watch The Wiggles every day on Nick JR.   \n4  ENG-test-0004  My 13-year-old son recommended this book to me...   \n\n   similarity_score  \n0               0.0  \n1               0.0  \n2               0.0  \n3               0.0  \n4               0.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PairID</th>\n      <th>sentence2</th>\n      <th>similarity_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ENG-test-0000</td>\n      <td>Egypt: Muslim Brotherhood Stands Behind Morsi</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ENG-test-0001</td>\n      <td>Install the program, which is free to download...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ENG-test-0002</td>\n      <td>Pretty much the first thing people mentioned w...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ENG-test-0003</td>\n      <td>You can watch The Wiggles every day on Nick JR.</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ENG-test-0004</td>\n      <td>My 13-year-old son recommended this book to me...</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_df.drop('sentence2', axis=1, inplace=True)\ntest_df.drop('similarity_score', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-30T18:21:12.047181Z","iopub.execute_input":"2024-01-30T18:21:12.047628Z","iopub.status.idle":"2024-01-30T18:21:12.054597Z","shell.execute_reply.started":"2024-01-30T18:21:12.047597Z","shell.execute_reply":"2024-01-30T18:21:12.053413Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-30T18:21:15.712640Z","iopub.execute_input":"2024-01-30T18:21:15.713934Z","iopub.status.idle":"2024-01-30T18:21:15.723758Z","shell.execute_reply.started":"2024-01-30T18:21:15.713891Z","shell.execute_reply":"2024-01-30T18:21:15.722683Z"},"trusted":true},"execution_count":131,"outputs":[{"execution_count":131,"output_type":"execute_result","data":{"text/plain":"          PairID\n0  ENG-test-0000\n1  ENG-test-0001\n2  ENG-test-0002\n3  ENG-test-0003\n4  ENG-test-0004","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PairID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ENG-test-0000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ENG-test-0001</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ENG-test-0002</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ENG-test-0003</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ENG-test-0004</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_df['Pred_Score'] = scor_list\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-30T18:21:19.254007Z","iopub.execute_input":"2024-01-30T18:21:19.254748Z","iopub.status.idle":"2024-01-30T18:21:19.266351Z","shell.execute_reply.started":"2024-01-30T18:21:19.254716Z","shell.execute_reply":"2024-01-30T18:21:19.265154Z"},"trusted":true},"execution_count":132,"outputs":[{"execution_count":132,"output_type":"execute_result","data":{"text/plain":"          PairID  Pred_Score\n0  ENG-test-0000        0.64\n1  ENG-test-0001        0.82\n2  ENG-test-0002        0.37\n3  ENG-test-0003        0.02\n4  ENG-test-0004        0.31","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PairID</th>\n      <th>Pred_Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ENG-test-0000</td>\n      <td>0.64</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ENG-test-0001</td>\n      <td>0.82</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ENG-test-0002</td>\n      <td>0.37</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ENG-test-0003</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ENG-test-0004</td>\n      <td>0.31</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_df.to_csv('/kaggle/working/pred_eng_a.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-30T18:21:23.602202Z","iopub.execute_input":"2024-01-30T18:21:23.602612Z","iopub.status.idle":"2024-01-30T18:21:23.614778Z","shell.execute_reply.started":"2024-01-30T18:21:23.602583Z","shell.execute_reply":"2024-01-30T18:21:23.613703Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}