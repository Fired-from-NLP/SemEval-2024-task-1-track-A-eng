{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7229554,"sourceType":"datasetVersion","datasetId":4185678},{"sourceId":7256669,"sourceType":"datasetVersion","datasetId":4205123},{"sourceId":7268101,"sourceType":"datasetVersion","datasetId":4213084},{"sourceId":7435370,"sourceType":"datasetVersion","datasetId":4327250},{"sourceId":7487113,"sourceType":"datasetVersion","datasetId":4358897}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install transformers sentence-transformers datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-28T14:14:06.000721Z","iopub.execute_input":"2024-01-28T14:14:06.001769Z","iopub.status.idle":"2024-01-28T14:14:19.447794Z","shell.execute_reply.started":"2024-01-28T14:14:06.001731Z","shell.execute_reply":"2024-01-28T14:14:19.446470Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.36.0)\nRequirement already satisfied: sentence-transformers in /opt/conda/lib/python3.10/site-packages (2.2.2)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.15.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.4)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.1.99)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.12.2)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.5)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->sentence-transformers) (1.16.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence-transformers) (10.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer, models\nfrom transformers import BertTokenizer\nfrom transformers import get_linear_schedule_with_warmup\nimport torch\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport time\nimport datetime\nimport random\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-01-28T14:14:19.450147Z","iopub.execute_input":"2024-01-28T14:14:19.450525Z","iopub.status.idle":"2024-01-28T14:14:19.458426Z","shell.execute_reply.started":"2024-01-28T14:14:19.450493Z","shell.execute_reply":"2024-01-28T14:14:19.457208Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-01-28T14:14:19.459934Z","iopub.execute_input":"2024-01-28T14:14:19.460340Z","iopub.status.idle":"2024-01-28T14:14:19.471212Z","shell.execute_reply.started":"2024-01-28T14:14:19.460304Z","shell.execute_reply":"2024-01-28T14:14:19.470209Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"There are 1 GPU(s) available.\nWe will use the GPU: Tesla P100-PCIE-16GB\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the English version of the STSB dataset\ndataset = load_dataset(\"stsb_multi_mt\", \"en\")\nprint(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T14:14:19.473487Z","iopub.execute_input":"2024-01-28T14:14:19.473846Z","iopub.status.idle":"2024-01-28T14:14:19.777501Z","shell.execute_reply.started":"2024-01-28T14:14:19.473805Z","shell.execute_reply":"2024-01-28T14:14:19.776388Z"},"trusted":true},"execution_count":70,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18094738ff504e57895a87c6c1f9d9ab"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['sentence1', 'sentence2', 'similarity_score'],\n        num_rows: 5749\n    })\n    test: Dataset({\n        features: ['sentence1', 'sentence2', 'similarity_score'],\n        num_rows: 1379\n    })\n    dev: Dataset({\n        features: ['sentence1', 'sentence2', 'similarity_score'],\n        num_rows: 1500\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"len(dataset['train'])","metadata":{"execution":{"iopub.status.busy":"2024-01-28T14:14:19.779055Z","iopub.execute_input":"2024-01-28T14:14:19.779414Z","iopub.status.idle":"2024-01-28T14:14:19.787063Z","shell.execute_reply.started":"2024-01-28T14:14:19.779379Z","shell.execute_reply":"2024-01-28T14:14:19.785956Z"},"trusted":true},"execution_count":71,"outputs":[{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"5749"},"metadata":{}}]},{"cell_type":"code","source":"print(\"A sample from the STSB dataset's training split:\")\nprint(dataset['train'][98])","metadata":{"execution":{"iopub.status.busy":"2024-01-28T14:14:19.788320Z","iopub.execute_input":"2024-01-28T14:14:19.788675Z","iopub.status.idle":"2024-01-28T14:14:19.800666Z","shell.execute_reply.started":"2024-01-28T14:14:19.788642Z","shell.execute_reply":"2024-01-28T14:14:19.799671Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"A sample from the STSB dataset's training split:\n{'sentence1': 'A man is slicing potatoes.', 'sentence2': 'A woman is peeling potato.', 'similarity_score': 2.200000047683716}\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/eng-train2/eng_train.csv')","metadata":{"execution":{"iopub.status.busy":"2024-01-28T14:14:19.802066Z","iopub.execute_input":"2024-01-28T14:14:19.802392Z","iopub.status.idle":"2024-01-28T14:14:19.831411Z","shell.execute_reply.started":"2024-01-28T14:14:19.802366Z","shell.execute_reply":"2024-01-28T14:14:19.830519Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-28T14:14:19.832503Z","iopub.execute_input":"2024-01-28T14:14:19.832804Z","iopub.status.idle":"2024-01-28T14:14:19.845164Z","shell.execute_reply.started":"2024-01-28T14:14:19.832778Z","shell.execute_reply":"2024-01-28T14:14:19.844006Z"},"trusted":true},"execution_count":74,"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"           PairID                                               Text  Score\n0  ENG-train-0000  It that happens, just pull the plug.\\nif that ...    1.0\n1  ENG-train-0001  A black dog running through water.\\nA black do...    1.0\n2  ENG-train-0002  I've been searchingthe entire abbey for you.\\n...    1.0\n3  ENG-train-0003  If he is good looking and has a good personali...    1.0\n4  ENG-train-0004  She does not hate you, she is just annoyed wit...    1.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PairID</th>\n      <th>Text</th>\n      <th>Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ENG-train-0000</td>\n      <td>It that happens, just pull the plug.\\nif that ...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ENG-train-0001</td>\n      <td>A black dog running through water.\\nA black do...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ENG-train-0002</td>\n      <td>I've been searchingthe entire abbey for you.\\n...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ENG-train-0003</td>\n      <td>If he is good looking and has a good personali...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ENG-train-0004</td>\n      <td>She does not hate you, she is just annoyed wit...</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Split the 'Text' column and create a DataFrame from the resulting list of lists\nsplit_text = df['Text'].apply(lambda x: x.split('\\n', 1)).to_list()\nsplit_text_df = pd.DataFrame(split_text, columns=['sentence1', 'sentence2'])\n\n# Assign the columns from the split_text_df to the original DataFrame (df)\ndf[['sentence1', 'sentence2']] = split_text_df[['sentence1', 'sentence2']]\n","metadata":{"execution":{"iopub.status.busy":"2024-01-28T14:14:19.846438Z","iopub.execute_input":"2024-01-28T14:14:19.846731Z","iopub.status.idle":"2024-01-28T14:14:19.864990Z","shell.execute_reply.started":"2024-01-28T14:14:19.846706Z","shell.execute_reply":"2024-01-28T14:14:19.864077Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-28T14:14:19.869450Z","iopub.execute_input":"2024-01-28T14:14:19.870072Z","iopub.status.idle":"2024-01-28T14:14:19.882986Z","shell.execute_reply.started":"2024-01-28T14:14:19.870021Z","shell.execute_reply":"2024-01-28T14:14:19.881967Z"},"trusted":true},"execution_count":76,"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"           PairID                                               Text  Score  \\\n0  ENG-train-0000  It that happens, just pull the plug.\\nif that ...    1.0   \n1  ENG-train-0001  A black dog running through water.\\nA black do...    1.0   \n2  ENG-train-0002  I've been searchingthe entire abbey for you.\\n...    1.0   \n3  ENG-train-0003  If he is good looking and has a good personali...    1.0   \n4  ENG-train-0004  She does not hate you, she is just annoyed wit...    1.0   \n\n                                           sentence1  \\\n0               It that happens, just pull the plug.   \n1                 A black dog running through water.   \n2       I've been searchingthe entire abbey for you.   \n3  If he is good looking and has a good personali...   \n4  She does not hate you, she is just annoyed wit...   \n\n                                           sentence2  \n0          if that ever happens, just pull the plug.  \n1         A black dog is running through some water.  \n2            I'm looking for you all over the abbey.  \n3  If he's good looking, and a good personality, ...  \n4         She doesn't hate you, she is just annoyed.  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PairID</th>\n      <th>Text</th>\n      <th>Score</th>\n      <th>sentence1</th>\n      <th>sentence2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ENG-train-0000</td>\n      <td>It that happens, just pull the plug.\\nif that ...</td>\n      <td>1.0</td>\n      <td>It that happens, just pull the plug.</td>\n      <td>if that ever happens, just pull the plug.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ENG-train-0001</td>\n      <td>A black dog running through water.\\nA black do...</td>\n      <td>1.0</td>\n      <td>A black dog running through water.</td>\n      <td>A black dog is running through some water.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ENG-train-0002</td>\n      <td>I've been searchingthe entire abbey for you.\\n...</td>\n      <td>1.0</td>\n      <td>I've been searchingthe entire abbey for you.</td>\n      <td>I'm looking for you all over the abbey.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ENG-train-0003</td>\n      <td>If he is good looking and has a good personali...</td>\n      <td>1.0</td>\n      <td>If he is good looking and has a good personali...</td>\n      <td>If he's good looking, and a good personality, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ENG-train-0004</td>\n      <td>She does not hate you, she is just annoyed wit...</td>\n      <td>1.0</td>\n      <td>She does not hate you, she is just annoyed wit...</td>\n      <td>She doesn't hate you, she is just annoyed.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.drop('PairID', axis = 1, inplace = True)\ndf.drop('Text', axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T14:14:19.884233Z","iopub.execute_input":"2024-01-28T14:14:19.884537Z","iopub.status.idle":"2024-01-28T14:14:19.894457Z","shell.execute_reply.started":"2024-01-28T14:14:19.884511Z","shell.execute_reply":"2024-01-28T14:14:19.893456Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"df['similarity_score'] = df['Score'] * 5.0\ndf.drop('Score', axis=1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T14:14:19.895722Z","iopub.execute_input":"2024-01-28T14:14:19.896070Z","iopub.status.idle":"2024-01-28T14:14:19.908530Z","shell.execute_reply.started":"2024-01-28T14:14:19.896041Z","shell.execute_reply":"2024-01-28T14:14:19.907561Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"df.head()\nprint(len(df))","metadata":{"execution":{"iopub.status.busy":"2024-01-28T14:14:19.909867Z","iopub.execute_input":"2024-01-28T14:14:19.910186Z","iopub.status.idle":"2024-01-28T14:14:19.919807Z","shell.execute_reply.started":"2024-01-28T14:14:19.910156Z","shell.execute_reply":"2024-01-28T14:14:19.918930Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stdout","text":"5500\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_df, dev_df = train_test_split(df, test_size=0.1, random_state = 1)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T14:14:19.920814Z","iopub.execute_input":"2024-01-28T14:14:19.921106Z","iopub.status.idle":"2024-01-28T14:14:19.932027Z","shell.execute_reply.started":"2024-01-28T14:14:19.921080Z","shell.execute_reply":"2024-01-28T14:14:19.931060Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"print(train_df.shape)\nprint(dev_df.shape)\nprint(len(dataset['train']))\nprint(dataset['train'][98])","metadata":{"execution":{"iopub.status.busy":"2024-01-28T14:14:19.933332Z","iopub.execute_input":"2024-01-28T14:14:19.933664Z","iopub.status.idle":"2024-01-28T14:14:19.943927Z","shell.execute_reply.started":"2024-01-28T14:14:19.933634Z","shell.execute_reply":"2024-01-28T14:14:19.942835Z"},"trusted":true},"execution_count":81,"outputs":[{"name":"stdout","text":"(4950, 3)\n(550, 3)\n5749\n{'sentence1': 'A man is slicing potatoes.', 'sentence2': 'A woman is peeling potato.', 'similarity_score': 2.200000047683716}\n","output_type":"stream"}]},{"cell_type":"code","source":"# sf=pd.read_csv('/kaggle/input/ansdata/eng_dev_with_labels.csv')\n\n# split_text = sf['Text'].apply(lambda x: x.split('\\n', 1)).to_list()\n# split_text_sf = pd.DataFrame(split_text, columns=['sentence1', 'sentence2'])\n\n# # Assign the columns from the split_text_df to the original DataFrame (df)\n# sf[['sentence1', 'sentence2']] = split_text_df[['sentence1', 'sentence2']]\n\n# sf.drop('PairID', axis = 1, inplace = True)\n# sf.drop('Text', axis = 1, inplace = True)\n\n# sf['similarity_score'] = sf['Score'] * 5.0\n# sf.drop('Score', axis=1, inplace = True)\n\n# sf.head()\n# sf.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-28T14:14:19.945059Z","iopub.execute_input":"2024-01-28T14:14:19.945366Z","iopub.status.idle":"2024-01-28T14:14:19.953873Z","shell.execute_reply.started":"2024-01-28T14:14:19.945338Z","shell.execute_reply":"2024-01-28T14:14:19.952878Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"# kf=[dev_df,sf]\n# dev_df = pd.concat(kf)\n# dev_df.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-28T14:14:19.955166Z","iopub.execute_input":"2024-01-28T14:14:19.955495Z","iopub.status.idle":"2024-01-28T14:14:19.963977Z","shell.execute_reply.started":"2024-01-28T14:14:19.955467Z","shell.execute_reply":"2024-01-28T14:14:19.963094Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"combined  = pd.read_csv('/kaggle/input/d/sajid064/combined/combined.csv')\ntrain_com_df, dev_com_df = train_test_split(combined, test_size=0.1, random_state = 1)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T14:14:19.965175Z","iopub.execute_input":"2024-01-28T14:14:19.965441Z","iopub.status.idle":"2024-01-28T14:14:20.020599Z","shell.execute_reply.started":"2024-01-28T14:14:19.965417Z","shell.execute_reply":"2024-01-28T14:14:20.019698Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"print(train_com_df.shape)\nprint(dev_com_df.shape)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T14:14:20.022312Z","iopub.execute_input":"2024-01-28T14:14:20.022683Z","iopub.status.idle":"2024-01-28T14:14:20.027852Z","shell.execute_reply.started":"2024-01-28T14:14:20.022645Z","shell.execute_reply":"2024-01-28T14:14:20.026654Z"},"trusted":true},"execution_count":85,"outputs":[{"name":"stdout","text":"(18362, 3)\n(2041, 3)\n","output_type":"stream"}]},{"cell_type":"code","source":"print(dataset['train'][0]['sentence1'])\n\nbaal1 = list(train_df['sentence1'])\nbaal2 = list(train_df['sentence2'])\nbaal3 = list(train_df['similarity_score'])\n\nbaal11 = list(train_com_df['sentence1'])\nbaal22 = list(train_com_df['sentence2'])\nbaal33 = list(train_com_df['similarity_score'])\n\nbaal1.extend(baal11)\nbaal2.extend(baal22)\nbaal3.extend(baal33)\n#updated_dataset = dataset['train'].map(lambda baal : {'sentence1': + baal1['baal']})\n\nprint(len(baal1))","metadata":{"execution":{"iopub.status.busy":"2024-01-28T14:14:20.029146Z","iopub.execute_input":"2024-01-28T14:14:20.029439Z","iopub.status.idle":"2024-01-28T14:14:20.053583Z","shell.execute_reply.started":"2024-01-28T14:14:20.029414Z","shell.execute_reply":"2024-01-28T14:14:20.052679Z"},"trusted":true},"execution_count":86,"outputs":[{"name":"stdout","text":"A plane is taking off.\n23312\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import DatasetDict, Dataset\n\ndataset['train'][0]['sentence1']\n\n#baal1 = list(train_df['sentence1'])\n#baal2 = list(train_df['sentence2'])\n#baal3 = list(train_df['similarity_score'])\n\n#updated_train_data\ndataset2 = dataset.copy()\n#print(len(dataset['train']))\n# Create a new list of dictionaries with updated values\nupdated_train_data = []\n#print(len(updated_train_data))\n\nupdated_train_data = [{'sentence1': s1, 'sentence2': s2, 'similarity_score': score} for s1, s2, score in zip(baal1, baal2, baal3)]\n\nupdated_train_data1 = [dataset['train'][i] for i in range(len(dataset['train']))]\nprint(len(updated_train_data1))\n\nupdated_train_data1.extend(updated_train_data)\n# Assign the updated list back to dataset['train']\n\ndataset2['train'] = updated_train_data1\nprint(len(dataset2['train']))\n#print(len(updated_train_data))\n\n#for i in range(0, 5749):\n#    dataset2['train'][i] = dataset['train'][i].copy()\n\ndataset['train'] = dataset2['train'].copy()\n\nprint(len(dataset['train']))","metadata":{"execution":{"iopub.status.busy":"2024-01-28T14:14:20.055170Z","iopub.execute_input":"2024-01-28T14:14:20.055892Z","iopub.status.idle":"2024-01-28T14:14:20.757003Z","shell.execute_reply.started":"2024-01-28T14:14:20.055853Z","shell.execute_reply":"2024-01-28T14:14:20.755939Z"},"trusted":true},"execution_count":87,"outputs":[{"name":"stdout","text":"5749\n29061\n29061\n","output_type":"stream"}]},{"cell_type":"code","source":"baal1 = list(dev_df['sentence1'])\nbaal2 = list(dev_df['sentence2'])\nbaal3 = list(dev_df['similarity_score'])\n\nbaal11 = list(dev_com_df['sentence1'])\nbaal22 = list(dev_com_df['sentence2'])\nbaal33 = list(dev_com_df['similarity_score'])\n\nbaal1.extend(baal11)\nbaal2.extend(baal22)\nbaal3.extend(baal33)\n\n\n#print(len(baal1))\nupdated_dev_data = []\n# Create a new list of dictionaries with updated values\nupdated_dev_data = [{'sentence1': s1, 'sentence2': s2, 'similarity_score': score} for s1, s2, score in zip(baal1, baal2, baal3)]\nupdated_dev_data1 = [dataset['dev'][i] for i in range(len(dataset['dev']))]\n\n#print(len(updated_dev_data))\n#print(len(updated_dev_data1))\n\nupdated_dev_data1.extend(updated_dev_data)\nprint(len(updated_dev_data1))\n# Assign the updated list back to dataset['train']\n\ndataset2['dev'] = updated_dev_data1\n#print(len(updated_dev_data))\n#print(len(dataset['dev']))\n#for i in range(0,1500):\n#    dataset2['dev'][i] = dataset['dev'][i].copy()\n\ndataset['dev'] = dataset2['dev'].copy()\n    \nprint(len(dataset['dev']))","metadata":{"execution":{"iopub.status.busy":"2024-01-28T14:14:20.758198Z","iopub.execute_input":"2024-01-28T14:14:20.758520Z","iopub.status.idle":"2024-01-28T14:14:20.887938Z","shell.execute_reply.started":"2024-01-28T14:14:20.758494Z","shell.execute_reply":"2024-01-28T14:14:20.886839Z"},"trusted":true},"execution_count":88,"outputs":[{"name":"stdout","text":"4091\n4091\n","output_type":"stream"}]},{"cell_type":"code","source":"# You can use larger variants of the model, here we're using the base model\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2024-01-28T14:14:20.889241Z","iopub.execute_input":"2024-01-28T14:14:20.889553Z","iopub.status.idle":"2024-01-28T14:14:21.058874Z","shell.execute_reply.started":"2024-01-28T14:14:20.889525Z","shell.execute_reply":"2024-01-28T14:14:21.057943Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"class STSBDataset(torch.utils.data.Dataset):\n\n    def __init__(self, dataset):\n        # Normalize the similarity scores in the dataset\n        similarity_scores = [i['similarity_score'] for i in dataset]\n        self.normalized_similarity_scores = [i/5.0 for i in similarity_scores]\n        self.first_sentences = [i['sentence1'] for i in dataset]\n        self.second_sentences = [i['sentence2'] for i in dataset]\n        self.concatenated_sentences = [[str(x), str(y)] for x,y in   zip(self.first_sentences, self.second_sentences)]\n\n    def __len__(self):\n        return len(self.concatenated_sentences)\n\n    def get_batch_labels(self, idx):\n        return torch.tensor(self.normalized_similarity_scores[idx])\n\n    def get_batch_texts(self, idx):\n        return tokenizer(self.concatenated_sentences[idx], padding='max_length', max_length=128, truncation=True, return_tensors=\"pt\")\n\n    def __getitem__(self, idx):\n        batch_texts = self.get_batch_texts(idx)\n        batch_y = self.get_batch_labels(idx)\n        return batch_texts, batch_y\n\n\n\ndef collate_fn(texts):\n    input_ids = texts['input_ids']\n    attention_masks = texts['attention_mask']\n    features = [{'input_ids': input_id, 'attention_mask': attention_mask}\n                for input_id, attention_mask in zip(input_ids, attention_masks)]\n    return features","metadata":{"execution":{"iopub.status.busy":"2024-01-28T14:14:21.060200Z","iopub.execute_input":"2024-01-28T14:14:21.060526Z","iopub.status.idle":"2024-01-28T14:14:21.071586Z","shell.execute_reply.started":"2024-01-28T14:14:21.060498Z","shell.execute_reply":"2024-01-28T14:14:21.070428Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"class BertForSTS(torch.nn.Module):\n\n    def __init__(self):\n        super(BertForSTS, self).__init__()\n        self.bert = models.Transformer('bert-base-uncased', max_seq_length=128)\n        self.pooling_layer = models.Pooling(self.bert.get_word_embedding_dimension())\n        self.sts_bert = SentenceTransformer(modules=[self.bert, self.pooling_layer])\n\n    def forward(self, input_data):\n        output = self.sts_bert(input_data)['sentence_embedding']\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-01-28T14:14:21.072933Z","iopub.execute_input":"2024-01-28T14:14:21.073251Z","iopub.status.idle":"2024-01-28T14:14:21.087142Z","shell.execute_reply.started":"2024-01-28T14:14:21.073225Z","shell.execute_reply":"2024-01-28T14:14:21.086165Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"# Instantiate the model and move it to GPU\nmodel = BertForSTS()\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T14:14:21.088454Z","iopub.execute_input":"2024-01-28T14:14:21.088759Z","iopub.status.idle":"2024-01-28T14:14:21.576684Z","shell.execute_reply.started":"2024-01-28T14:14:21.088729Z","shell.execute_reply":"2024-01-28T14:14:21.575681Z"},"trusted":true},"execution_count":92,"outputs":[{"execution_count":92,"output_type":"execute_result","data":{"text/plain":"BertForSTS(\n  (bert): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n  (pooling_layer): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n  (sts_bert): SentenceTransformer(\n    (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n    (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"class CosineSimilarityLoss(torch.nn.Module):\n\n    def __init__(self,  loss_fn=torch.nn.MSELoss(), transform_fn=torch.nn.Identity()):\n        super(CosineSimilarityLoss, self).__init__()\n        self.loss_fn = loss_fn\n        self.transform_fn = transform_fn\n        self.cos_similarity = torch.nn.CosineSimilarity(dim=1)\n\n    def forward(self, inputs, labels):\n        emb_1 = torch.stack([inp[0] for inp in inputs])\n        emb_2 = torch.stack([inp[1] for inp in inputs])\n        outputs = self.transform_fn(self.cos_similarity(emb_1, emb_2))\n        return self.loss_fn(outputs, labels.squeeze())","metadata":{"execution":{"iopub.status.busy":"2024-01-28T14:14:21.578444Z","iopub.execute_input":"2024-01-28T14:14:21.578842Z","iopub.status.idle":"2024-01-28T14:14:21.586767Z","shell.execute_reply.started":"2024-01-28T14:14:21.578807Z","shell.execute_reply":"2024-01-28T14:14:21.585847Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"train_ds = STSBDataset(dataset['train'])\nval_ds = STSBDataset(dataset['dev'])\n\n# Create a 90-10 train-validation split.\ntrain_size = len(train_ds)\nval_size = len(val_ds)\n\nprint('{:>5,} training samples'.format(train_size))\nprint('{:>5,} validation samples'.format(val_size))","metadata":{"execution":{"iopub.status.busy":"2024-01-28T14:14:21.592911Z","iopub.execute_input":"2024-01-28T14:14:21.593259Z","iopub.status.idle":"2024-01-28T14:14:21.638146Z","shell.execute_reply.started":"2024-01-28T14:14:21.593232Z","shell.execute_reply":"2024-01-28T14:14:21.637125Z"},"trusted":true},"execution_count":94,"outputs":[{"name":"stdout","text":"29,061 training samples\n4,091 validation samples\n","output_type":"stream"}]},{"cell_type":"code","source":"train_ds[0]","metadata":{"execution":{"iopub.status.busy":"2024-01-28T14:14:21.639405Z","iopub.execute_input":"2024-01-28T14:14:21.639855Z","iopub.status.idle":"2024-01-28T14:14:21.655018Z","shell.execute_reply.started":"2024-01-28T14:14:21.639816Z","shell.execute_reply":"2024-01-28T14:14:21.653949Z"},"trusted":true},"execution_count":95,"outputs":[{"execution_count":95,"output_type":"execute_result","data":{"text/plain":"({'input_ids': tensor([[ 101, 1037, 4946, 2003, 2635, 2125, 1012,  102,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0],\n         [ 101, 2019, 2250, 4946, 2003, 2635, 2125, 1012,  102,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n             0,    0,    0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0],\n         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0],\n         [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0]])},\n tensor(1.))"},"metadata":{}}]},{"cell_type":"code","source":"batch_size = 8\n\ntrain_dataloader = DataLoader(\n            train_ds,  # The training samples.\n            num_workers = 4,\n            batch_size = batch_size, # Use this batch size.\n            shuffle=True # Select samples randomly for each batch\n        )\n\nvalidation_dataloader = DataLoader(\n            val_ds,\n            num_workers = 4,\n            batch_size = batch_size # Use the same batch size\n        )","metadata":{"execution":{"iopub.status.busy":"2024-01-28T14:14:21.656396Z","iopub.execute_input":"2024-01-28T14:14:21.656746Z","iopub.status.idle":"2024-01-28T14:14:21.669593Z","shell.execute_reply.started":"2024-01-28T14:14:21.656718Z","shell.execute_reply":"2024-01-28T14:14:21.668409Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(),\n                  lr = 9e-6)\nepochs = 12\n# Total number of training steps is [number of batches] x [number of epochs].\ntotal_steps = len(train_dataloader) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer,\n                                            num_warmup_steps = 0,\n                                            num_training_steps = total_steps)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-28T14:14:21.671207Z","iopub.execute_input":"2024-01-28T14:14:21.671651Z","iopub.status.idle":"2024-01-28T14:14:21.681462Z","shell.execute_reply.started":"2024-01-28T14:14:21.671595Z","shell.execute_reply":"2024-01-28T14:14:21.680664Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"# Define format_time function\nimport time\n\ndef format_time(elapsed):\n    \"\"\"\n    Takes a time in seconds and returns a string hh:mm:ss\n    \"\"\"\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_rounded)))","metadata":{"execution":{"iopub.status.busy":"2024-01-28T14:14:21.682932Z","iopub.execute_input":"2024-01-28T14:14:21.683326Z","iopub.status.idle":"2024-01-28T14:14:21.692783Z","shell.execute_reply.started":"2024-01-28T14:14:21.683292Z","shell.execute_reply":"2024-01-28T14:14:21.691666Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"def train():\n  seed_val = 42\n  criterion = CosineSimilarityLoss()\n  criterion = criterion.cuda()\n  random.seed(seed_val)\n  torch.manual_seed(seed_val)\n  # We'll store a number of quantities such as training and validation loss,\n  # validation accuracy, and timings.\n  training_stats = []\n  total_t0 = time.time()\n  for epoch_i in range(0, epochs):\n      t0 = time.time()\n      total_train_loss = 0\n      model.train()\n      # For each batch of training data...\n      for train_data, train_label in tqdm(train_dataloader):\n          train_data['input_ids'] = train_data['input_ids'].to(device)\n          train_data['attention_mask'] = train_data['attention_mask'].to(device)\n          train_data = collate_fn(train_data)\n          model.zero_grad()\n          output = [model(feature) for feature in train_data]\n          loss = criterion(output, train_label.to(device))\n          total_train_loss += loss.item()\n          loss.backward()\n          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n          optimizer.step()\n          scheduler.step()\n\n      # Calculate the average loss over all of the batches.\n      avg_train_loss = total_train_loss / len(train_dataloader)\n      # Measure how long this epoch took.\n      training_time = format_time(time.time() - t0)\n      t0 = time.time()\n      model.eval()\n      total_eval_accuracy = 0\n      total_eval_loss = 0\n      nb_eval_steps = 0\n      # Evaluate data for one epoch\n      for val_data, val_label in tqdm(validation_dataloader):\n          val_data['input_ids'] = val_data['input_ids'].to(device)\n          val_data['attention_mask'] = val_data['attention_mask'].to(device)\n          val_data = collate_fn(val_data)\n          with torch.no_grad():\n              output = [model(feature) for feature in val_data]\n          loss = criterion(output, val_label.to(device))\n          total_eval_loss += loss.item()\n      # Calculate the average loss over all of the batches.\n      avg_val_loss = total_eval_loss / len(validation_dataloader)\n      # Measure how long the validation run took.\n      validation_time = format_time(time.time() - t0)\n      # Record all statistics from this epoch.\n      training_stats.append(\n          {\n              'epoch': epoch_i + 1,\n              'Training Loss': avg_train_loss,\n              'Valid. Loss': avg_val_loss,\n              'Training Time': training_time,\n              'Validation Time': validation_time\n          }\n      )\n  return model, training_stats\n\n# Launch the training\nmodel, training_stats = train()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-28T14:14:21.694181Z","iopub.execute_input":"2024-01-28T14:14:21.694536Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"100%|██████████| 3633/3633 [19:24<00:00,  3.12it/s]\n100%|██████████| 512/512 [00:51<00:00,  9.95it/s]\n100%|██████████| 3633/3633 [19:29<00:00,  3.11it/s]\n100%|██████████| 512/512 [00:51<00:00,  9.85it/s]\n100%|██████████| 3633/3633 [19:32<00:00,  3.10it/s]\n100%|██████████| 512/512 [00:51<00:00,  9.88it/s]\n100%|██████████| 3633/3633 [19:32<00:00,  3.10it/s]\n100%|██████████| 512/512 [00:51<00:00,  9.85it/s]\n100%|██████████| 3633/3633 [19:37<00:00,  3.09it/s]\n100%|██████████| 512/512 [00:51<00:00,  9.88it/s]\n100%|██████████| 3633/3633 [19:27<00:00,  3.11it/s]\n 36%|███▋      | 186/512 [00:19<00:32,  9.91it/s]","output_type":"stream"}]},{"cell_type":"code","source":"# Create a DataFrame from our training statistics\ndf_stats = pd.DataFrame(data=training_stats)\n\n# Use the 'epoch' as the row index\ndf_stats = df_stats.set_index('epoch')\n\n# Display the table\ndf_stats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the test set\ntest_dataset = load_dataset(\"stsb_multi_mt\", name=\"en\", split=\"test\")\n\n\ntest_df = pd.read_csv('/kaggle/input/eng-train2/eng_dev.csv')\n#print(test_dataset)\n\ntest_df[['sentence1', 'sentence2']] = test_df['Text'].apply(lambda x: x.split('\\n', 1)).to_list()\ntest_df.drop('Text', axis=1, inplace=True)\n#dev_df.drop('PairID', axis=1, inplace=True)\ntest_df['similarity_score'] = 0.0\nbaal1 = list(test_df['sentence1'])\nbaal2 = list(test_df['sentence2'])\nbaal3 = list(test_df['similarity_score'])\n# Create a new list of dictionaries with updated values\nupdated_train_data = [{'sentence1': s1, 'sentence2': s2, 'similarity_score': s3} for s1, s2, s3 in zip(baal1, baal2, baal3)]\n\n# Assign the updated list back to dataset['train']\ntest_dataset = updated_train_data\n\n\n# Prepare the data\nfirst_sent = [i['sentence1'] for i in test_dataset]\nsecond_sent = [i['sentence2'] for i in test_dataset]\nfull_text = [[str(x), str(y)] for x,y in zip(first_sent, second_sent)]\n\nprint(len(full_text))\n\nmodel.eval()\n\ndef predict_similarity(sentence_pair):\n  test_input = tokenizer(sentence_pair, padding='max_length', max_length = 128, truncation=True, return_tensors=\"pt\").to(device)\n  test_input['input_ids'] = test_input['input_ids']\n  test_input['attention_mask'] = test_input['attention_mask']\n  del test_input['token_type_ids']\n  output = model(test_input)\n  sim = torch.nn.functional.cosine_similarity(output[0], output[1], dim=0).item()\n  return sim","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_1 = full_text[0]\nprint(f\"Sentence 1: {example_1[0]}\")\nprint(f\"Sentence 2: {example_1[1]}\")\nprint(f\"Predicted similarity score: {round(predict_similarity(example_1), 2)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scor_list = []\nfor i in full_text:\n    score = round(predict_similarity(i), 2)\n    if(score<0):\n        score = score * (-1)\n    print(score)\n    scor_list.append(score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.drop('sentence1', axis=1, inplace=True)\ntest_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.drop('sentence2', axis=1, inplace=True)\ntest_df.drop('similarity_score', axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['Pred_Score'] = scor_list\ntest_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.to_csv('pred_eng_a.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}